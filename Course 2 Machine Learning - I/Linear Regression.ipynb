{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193c6fbc-130d-4dff-ad9d-d8f10f2f2830",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc55eb-3b73-410c-94b5-a87e0f6e0884",
   "metadata": {},
   "source": [
    "##### Refer below link for simple linear regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a42af-a96e-4de7-8c33-c19e3713863b",
   "metadata": {},
   "source": [
    "https://www.mathsisfun.com/equation_of_line.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d9b99-69a2-442e-bb9e-1d3930431875",
   "metadata": {},
   "source": [
    "### Understanding How Best Fit line or Regression line's RSS and Line itself are caculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccacbf7-9b35-4878-9b39-0e16580b8427",
   "metadata": {},
   "source": [
    "Let's go step by step, using your hypothetical dataset of 10 rows of marketing spend (in lakhs) and sales value (in crores) to explain how linear regression works, how we calculate the slope and intercept, why we use **one** slope and intercept for the entire dataset, and how the Residual Sum of Squares (RSS) is minimized.\n",
    "\n",
    "### Example Dataset:\n",
    "| Day | Marketing Spend (X) in Lakhs | Sales (Y) in Crores |\n",
    "|-----|------------------------------|---------------------|\n",
    "| 1   | 1                            | 2                   |\n",
    "| 2   | 2                            | 4                   |\n",
    "| 3   | 3                            | 5                   |\n",
    "| 4   | 4                            | 4.5                 |\n",
    "| 5   | 5                            | 6                   |\n",
    "| 6   | 6                            | 7                   |\n",
    "| 7   | 7                            | 7.5                 |\n",
    "| 8   | 8                            | 9                   |\n",
    "| 9   | 9                            | 10                  |\n",
    "| 10  | 10                           | 12                  |\n",
    "\n",
    "### 1. **Understanding the Goal of Linear Regression**:\n",
    "The goal of linear regression is to find a **line of best fit** that predicts the dependent variable (Sales, \\(Y\\)) based on the independent variable (Marketing Spend, \\(X\\)). The line should minimize the difference between the actual values (\\(Y\\)) and the predicted values from the line (\\(\\hat{Y}\\)).\n",
    "\n",
    "The general form of the linear regression equation is:\n",
    "\\[\n",
    "\\hat{Y} = mX + c\n",
    "\\]\n",
    "Where:\n",
    "- \\(m\\) = slope of the line (how much sales increase when marketing spend increases by 1 lakh),\n",
    "- \\(c\\) = intercept (the sales when marketing spend is 0),\n",
    "- \\(X\\) = marketing spend in lakhs,\n",
    "- \\(\\hat{Y}\\) = predicted sales in crores.\n",
    "\n",
    "### 2. **Why Do We Use One Slope and Intercept?**:\n",
    "Linear regression calculates **one slope and one intercept** that best fits **all** the data points in the dataset, not just two consecutive points. Here's why:\n",
    "- If you calculate a slope between every pair of consecutive points, you’d end up with different slopes for each pair, leading to many different lines.\n",
    "- The essence of linear regression is to find the **one line** that best describes the relationship between \\(X\\) and \\(Y\\) for the entire dataset, not just locally between two points.\n",
    "- Using one slope and intercept ensures a **global relationship** between \\(X\\) and \\(Y\\).\n",
    "\n",
    "### 3. **How Do We Calculate Slope and Intercept?**:\n",
    "To calculate the slope (\\(m\\)) and intercept (\\(c\\)) for the **entire dataset**, we use a formula that takes into account **all** the points, not just two at a time.\n",
    "\n",
    "#### Slope Formula:\n",
    "\\[\n",
    "m = \\frac{n \\sum(XY) - \\sum X \\sum Y}{n \\sum X^2 - (\\sum X)^2}\n",
    "\\]\n",
    "Where:\n",
    "- \\(n\\) is the number of data points (here, \\(n = 10\\)),\n",
    "- \\(\\sum XY\\) is the sum of the product of corresponding \\(X\\) and \\(Y\\) values,\n",
    "- \\(\\sum X\\) and \\(\\sum Y\\) are the sums of \\(X\\) and \\(Y\\) values, respectively,\n",
    "- \\(\\sum X^2\\) is the sum of the squares of the \\(X\\) values.\n",
    "\n",
    "#### Intercept Formula:\n",
    "\\[\n",
    "c = \\frac{\\sum Y - m \\sum X}{n}\n",
    "\\]\n",
    "Once we calculate \\(m\\) and \\(c\\), we have the equation for the regression line, which can predict sales for any given marketing spend.\n",
    "\n",
    "#### Why One Slope and Intercept?\n",
    "- We want to capture the **overall trend** in the data, not just the local changes between two specific points. This is why we compute one slope and one intercept that represent the **entire dataset**.\n",
    "- The goal is to create a line that minimizes the total error (RSS) across **all** the points, not just locally for a pair of points.\n",
    "\n",
    "### 4. **Example Calculation**:\n",
    "Let’s calculate the slope and intercept for this dataset.\n",
    "\n",
    "#### Step 1: Calculate the needed sums:\n",
    "| Day | X  | Y  | XY   | X²  |\n",
    "|-----|----|----|------|-----|\n",
    "| 1   | 1  | 2  | 2    | 1   |\n",
    "| 2   | 2  | 4  | 8    | 4   |\n",
    "| 3   | 3  | 5  | 15   | 9   |\n",
    "| 4   | 4  | 4.5| 18   | 16  |\n",
    "| 5   | 5  | 6  | 30   | 25  |\n",
    "| 6   | 6  | 7  | 42   | 36  |\n",
    "| 7   | 7  | 7.5| 52.5 | 49  |\n",
    "| 8   | 8  | 9  | 72   | 64  |\n",
    "| 9   | 9  | 10 | 90   | 81  |\n",
    "| 10  | 10 | 12 | 120  | 100 |\n",
    "\n",
    "Now, compute the totals:\n",
    "- \\(\\sum X = 55\\),\n",
    "- \\(\\sum Y = 67\\),\n",
    "- \\(\\sum XY = 449.5\\),\n",
    "- \\(\\sum X^2 = 385\\),\n",
    "- \\(n = 10\\).\n",
    "\n",
    "#### Step 2: Calculate the slope \\(m\\):\n",
    "\\[\n",
    "m = \\frac{10(449.5) - (55)(67)}{10(385) - (55)^2} = \\frac{4495 - 3685}{3850 - 3025} = \\frac{810}{825} \\approx 0.982\n",
    "\\]\n",
    "So, the slope \\(m\\) is approximately 0.982.\n",
    "\n",
    "#### Step 3: Calculate the intercept \\(c\\):\n",
    "\\[\n",
    "c = \\frac{67 - (0.982)(55)}{10} = \\frac{67 - 54.01}{10} \\approx \\frac{12.99}{10} \\approx 1.299\n",
    "\\]\n",
    "So, the intercept \\(c\\) is approximately 1.299.\n",
    "\n",
    "#### Final Regression Line:\n",
    "\\[\n",
    "\\hat{Y} = 0.982X + 1.299\n",
    "\\]\n",
    "\n",
    "This equation means that:\n",
    "- For every additional **1 lakh** spent on marketing, the sales increase by **0.982 crores**.\n",
    "- If no marketing is spent (\\(X = 0\\)), the predicted sales would be approximately **1.299 crores**.\n",
    "\n",
    "### 5. **Residual Sum of Squares (RSS)**:\n",
    "The **RSS** is the total of the squared differences between the actual sales values (\\(Y_i\\)) and the predicted sales values (\\(\\hat{Y_i}\\)) for each point.\n",
    "\n",
    "For each data point:\n",
    "\\[\n",
    "\\text{Residual} = Y_i - \\hat{Y_i}\n",
    "\\]\n",
    "\n",
    "Then, you square each residual and sum them up:\n",
    "\\[\n",
    "\\text{RSS} = \\sum (Y_i - \\hat{Y_i})^2\n",
    "\\]\n",
    "\n",
    "The regression line is chosen to minimize this RSS, meaning the line that minimizes the total error between the actual and predicted sales.\n",
    "\n",
    "### 6. **Why Not Different Slopes for Each Point?**:\n",
    "- If we calculated a different slope for each pair of points, we’d end up with **many lines** instead of one.\n",
    "- The purpose of regression is to find the **best global fit**—one line that captures the overall relationship between \\(X\\) and \\(Y\\), rather than locally between two points.\n",
    "- This global approach makes predictions much more reliable because it smooths out individual fluctuations and noise in the data.\n",
    "\n",
    "### Conclusion:\n",
    "- The **regression line** is a summary of the relationship between marketing spend and sales over the entire dataset.\n",
    "- We calculate **one slope** and **one intercept** to ensure a consistent model that minimizes the **overall error** (RSS).\n",
    "- This approach helps us find a line that provides the best possible prediction for sales, based on marketing spend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6543cc-9424-4d70-b215-56982c8808f1",
   "metadata": {},
   "source": [
    "https://online.stat.psu.edu/stat462/node/91/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839b2d6-802c-45d9-a1b7-a1dd37b0bcdb",
   "metadata": {},
   "source": [
    "## To Find Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e8d1d6-42fa-4f73-9925-fc4f9a97cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data points\n",
    "x = [1, 2, 3]\n",
    "y = [2, 3, 4]\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b782a-5a52-4292-aab9-3816cfdd0a04",
   "metadata": {},
   "source": [
    "### To Learn about assumptions made while creating Linear Regression Model\n",
    "https://people.duke.edu/~rnau/testing.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dd135-e671-4dd4-b6eb-4066658173a0",
   "metadata": {},
   "source": [
    "https://help.reliasoft.com/reference/experiment_design_and_analysis/doe/simple_linear_regression_analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef693c-860a-48aa-82f8-379334a6f119",
   "metadata": {},
   "source": [
    "https://home.iitk.ac.in/~shalab/regression/Chapter2-Regression-SimpleLinearRegressionAnalysis.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f17ee6-b3d2-4090-9550-235bb2e8d6d0",
   "metadata": {},
   "source": [
    "https://home.iitk.ac.in/~shalab/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e07c01-2862-403e-8e4d-63dabf58c503",
   "metadata": {},
   "source": [
    "### To learn about F test\n",
    "https://en.wikipedia.org/wiki/F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8a45f-2ee6-433f-b035-b214e52b5711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d15770-b690-41fc-9e3f-dcd486487e78",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO MACHINE LEARNING OWN NOTES BY NISHANTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2ab8b-5cc8-4a49-a973-ec48a4d6730a",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence (AI) that involves creating systems that can learn from data, identify patterns, and make decisions with minimal human intervention. It is primarily focused on developing algorithms and models that allow computers to improve their performance on a task through experience.\n",
    "\n",
    "#### Key Concepts of Machine Learning\n",
    "\n",
    "1. **Data**: \n",
    "   - Data is the foundation of machine learning. It includes the information and variables the machine learning models will use to learn and make predictions. The data can be in many forms, such as numerical, textual, visual, or audio.\n",
    "\n",
    "2. **Features**:\n",
    "   - Features are individual independent variables that act as input to a model. They are measurable properties of the data, helping the model distinguish between various patterns.\n",
    "\n",
    "3. **Labels/Target Variables**:\n",
    "   - Labels (also called target variables) are the output or the ground truth of the data for supervised learning. In classification problems, labels might be \"spam\" or \"not spam,\" while in regression, they can be the predicted value, like a house price.\n",
    "\n",
    "4. **Training and Testing**:\n",
    "   - Data is often split into training and testing sets. The training set is used to train the model, while the testing set evaluates the model's performance.\n",
    "\n",
    "5. **Model**:\n",
    "   - A model is a mathematical representation created by a machine learning algorithm. It is used to find patterns and make predictions based on the data.\n",
    "\n",
    "6. **Algorithm**:\n",
    "   - An algorithm is a set of rules or instructions given to a machine to help it learn on its own. In machine learning, algorithms define how the model should be trained and fine-tuned.\n",
    "\n",
    "#### Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**:\n",
    "   - In supervised learning, the model is trained on a labeled dataset. This means that each training example is paired with an output label. The goal is to learn a mapping from inputs to outputs.\n",
    "   - **Examples**: \n",
    "     - Classification: Spam detection, sentiment analysis.\n",
    "     - Regression: Predicting house prices, stock prices.\n",
    "\n",
    "2. **Unsupervised Learning**:\n",
    "   - In unsupervised learning, the model is given data without labeled responses and is tasked with finding hidden patterns or structures.\n",
    "   - **Examples**:\n",
    "     - Clustering: Customer segmentation.\n",
    "     - Dimensionality Reduction: Principal Component Analysis (PCA).\n",
    "\n",
    "3. **Reinforcement Learning**:\n",
    "   - Reinforcement learning involves training an agent through trial and error to maximize a reward function. It is commonly used in areas like game playing and robotics.\n",
    "   - **Examples**: Game playing (like AlphaGo), robotics control.\n",
    "\n",
    "4. **Semi-Supervised Learning**:\n",
    "   - A combination of supervised and unsupervised learning, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.\n",
    "\n",
    "5. **Self-Supervised Learning**:\n",
    "   - A type of unsupervised learning where the model generates labels from the input data itself, often used for tasks like language modeling.\n",
    "\n",
    "#### Popular Machine Learning Algorithms\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - Used for regression problems, it models the relationship between dependent and independent variables.\n",
    "\n",
    "2. **Logistic Regression**:\n",
    "   - A classification algorithm used to predict binary outcomes.\n",
    "\n",
    "3. **Decision Trees**:\n",
    "   - A tree-like model used for both classification and regression tasks.\n",
    "\n",
    "4. **Support Vector Machines (SVM)**:\n",
    "   - Used for classification tasks, SVM finds the hyperplane that best separates the classes.\n",
    "\n",
    "5. **K-Nearest Neighbors (KNN)**:\n",
    "   - A simple algorithm that stores all available cases and classifies new cases based on a similarity measure.\n",
    "\n",
    "6. **Neural Networks**:\n",
    "   - Composed of interconnected nodes (neurons), neural networks are used for complex tasks like image and speech recognition.\n",
    "\n",
    "7. **Ensemble Methods**:\n",
    "   - Methods like Random Forest and Gradient Boosting that combine multiple models to produce a better result.\n",
    "\n",
    "#### Machine Learning Workflow\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Gathering raw data from various sources, which can be structured or unstructured.\n",
    "\n",
    "2. **Data Cleaning and Preprocessing**:\n",
    "   - Handling missing values, outliers, and normalizing data.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Selecting, extracting, and creating features that are relevant for the task.\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - Choosing a suitable algorithm based on the problem type.\n",
    "\n",
    "5. **Model Training**:\n",
    "   - Fitting the model to the training data.\n",
    "\n",
    "6. **Model Evaluation**:\n",
    "   - Using evaluation metrics (e.g., accuracy, precision, recall) to assess model performance.\n",
    "\n",
    "7. **Hyperparameter Tuning**:\n",
    "   - Fine-tuning the model parameters to improve performance.\n",
    "\n",
    "8. **Deployment and Monitoring**:\n",
    "   - Deploying the model in a production environment and monitoring its performance.\n",
    "\n",
    "### Applications of Machine Learning\n",
    "\n",
    "1. **Healthcare**: Disease prediction, medical imaging analysis.\n",
    "2. **Finance**: Fraud detection, risk assessment.\n",
    "3. **Marketing**: Customer segmentation, recommendation systems.\n",
    "4. **E-commerce**: Product recommendations, inventory management.\n",
    "5. **Natural Language Processing**: Sentiment analysis, language translation.\n",
    "6. **Autonomous Systems**: Self-driving cars, robotics.\n",
    "\n",
    "### Getting Started with Machine Learning\n",
    "\n",
    "If you're interested in diving deeper into machine learning, consider:\n",
    "\n",
    "1. **Learning Python Libraries**:\n",
    "   - `scikit-learn` for classic machine learning algorithms.\n",
    "   - `TensorFlow` and `PyTorch` for deep learning.\n",
    "   - `pandas` and `NumPy` for data manipulation and analysis.\n",
    "\n",
    "2. **Working with Datasets**:\n",
    "   - Start with publicly available datasets from [Kaggle](https://www.kaggle.com/) or the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).\n",
    "\n",
    "3. **Building and Deploying Models**:\n",
    "   - Experiment with small projects, such as creating a predictive model or building a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab11b79-5af2-419e-bc67-9e674f8aa50e",
   "metadata": {},
   "source": [
    "Let’s start with **Supervised Learning** and **Unsupervised Learning**:\n",
    "\n",
    "### 1. **Supervised Learning**\n",
    "Supervised learning involves training a model on labeled data, where the correct outputs (or labels) are already known. The model learns by making predictions and adjusting based on its errors until it achieves a desired level of accuracy.\n",
    "\n",
    "#### Key Concepts in Supervised Learning:\n",
    "- **Input Features**: The independent variables used to make predictions.\n",
    "- **Output Labels**: The target variable or labels.\n",
    "- **Training Data**: The dataset used to train the model, containing both input features and corresponding labels.\n",
    "- **Testing Data**: Separate data used to evaluate the model’s performance.\n",
    "\n",
    "#### Types of Supervised Learning Problems:\n",
    "1. **Classification**:\n",
    "   - Used to predict a category or class.\n",
    "   - **Examples**: Spam detection, image classification, customer segmentation.\n",
    "   - **Popular Algorithms**: Logistic Regression, Support Vector Machines, Decision Trees, Random Forest, k-Nearest Neighbors.\n",
    "\n",
    "2. **Regression**:\n",
    "   - Used to predict a continuous value.\n",
    "   - **Examples**: Predicting house prices, stock price forecasting.\n",
    "   - **Popular Algorithms**: Linear Regression, Polynomial Regression, Support Vector Regression, Decision Trees, Ridge and Lasso Regression.\n",
    "\n",
    "#### Example: Classification Using Decision Trees\n",
    "In a simple classification problem, say we want to predict whether a person will buy a particular product based on their age and income. Here, the **input features** are age and income, and the **label** is whether the person made a purchase (Yes/No).\n",
    "\n",
    "- The decision tree will learn rules like:\n",
    "  - If `age < 30` and `income = high`, then likely `No`.\n",
    "  - If `age >= 30` and `income = low`, then `Yes`.\n",
    "  \n",
    "The algorithm builds a tree-like model by repeatedly splitting the data based on feature values that result in the best separation of classes.\n",
    "\n",
    "#### Advantages of Supervised Learning:\n",
    "- Clear and precise predictions with well-defined labels.\n",
    "- Easy to evaluate using accuracy, precision, and other metrics.\n",
    "- Effective for many real-world applications like fraud detection, medical diagnosis, and customer segmentation.\n",
    "\n",
    "#### Challenges:\n",
    "- Requires a large amount of labeled data.\n",
    "- May not generalize well to new, unseen data (overfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Unsupervised Learning**\n",
    "Unsupervised learning deals with data that has no predefined labels or target outputs. The goal is to find hidden structures or patterns in the data without any guidance on what to look for.\n",
    "\n",
    "#### Key Concepts in Unsupervised Learning:\n",
    "- **Clusters**: Groups of data points that are similar to each other.\n",
    "- **Dimensionality Reduction**: Techniques to reduce the number of features in a dataset while preserving its core structure.\n",
    "\n",
    "#### Types of Unsupervised Learning Problems:\n",
    "1. **Clustering**:\n",
    "   - Groups similar data points together.\n",
    "   - **Examples**: Customer segmentation, image compression, topic modeling.\n",
    "   - **Popular Algorithms**: K-Means, Hierarchical Clustering, DBSCAN (Density-Based Spatial Clustering).\n",
    "\n",
    "2. **Dimensionality Reduction**:\n",
    "   - Reduces the number of features while retaining the essence of the data.\n",
    "   - **Examples**: Principal Component Analysis (PCA), t-SNE (t-distributed Stochastic Neighbor Embedding).\n",
    "   - **Use Case**: Visualizing high-dimensional data in 2D or 3D space.\n",
    "\n",
    "#### Example: Clustering Using K-Means\n",
    "Imagine a retail company wants to group customers based on purchasing behavior. The **input features** could be the amount spent on different product categories.\n",
    "\n",
    "- K-Means will randomly initialize `K` cluster centers.\n",
    "- It then assigns each customer to the nearest cluster based on their purchasing patterns.\n",
    "- The algorithm iteratively updates the cluster centers until it stabilizes, with customers grouped into segments that reflect similar buying behaviors.\n",
    "\n",
    "#### Advantages of Unsupervised Learning:\n",
    "- No need for labeled data, making it useful for discovering unknown patterns.\n",
    "- Can reveal hidden structures in data that aren’t obvious.\n",
    "\n",
    "#### Challenges:\n",
    "- Evaluating model quality can be tricky since there are no labels.\n",
    "- Results can vary depending on the choice of hyperparameters (e.g., the number of clusters in K-Means).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2996a-04df-423c-adb0-bf822f47fa97",
   "metadata": {},
   "source": [
    "### Introduction to Simple Linear Regression\n",
    "\n",
    "**Simple Linear Regression** is one of the most basic yet widely used algorithms in supervised learning. It models the relationship between two variables by fitting a straight line to the data. The algorithm assumes a linear relationship between the independent variable (predictor) and the dependent variable (response).\n",
    "\n",
    "#### **Understanding the Linear Regression Model**\n",
    "The simple linear regression equation is defined as:\n",
    "\n",
    "\\[\n",
    "Y = mX + c\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Y** is the **dependent variable** (target/output).\n",
    "- **X** is the **independent variable** (predictor/input).\n",
    "- **m** is the **slope** of the line (represents the change in Y when X increases by one unit).\n",
    "- **c** is the **intercept** (the value of Y when X is zero).\n",
    "\n",
    "The goal of the model is to learn values for `m` and `c` such that the straight line best fits the data points, minimizing the error between predicted and actual values of Y.\n",
    "\n",
    "#### **Visual Representation**\n",
    "\n",
    "Consider a dataset with `X` representing the number of hours studied and `Y` representing the score obtained:\n",
    "\n",
    "- The algorithm will try to find a line of best fit that shows the relationship between hours studied and score, predicting that as study hours increase, the score should increase proportionally.\n",
    "\n",
    "If plotted on a graph:\n",
    "\n",
    "- The **x-axis**: Number of hours studied.\n",
    "- The **y-axis**: Exam score.\n",
    "- The **line of best fit**: A straight line passing through the center of the data points, showing the trend.\n",
    "\n",
    "#### **Mathematical Formulation**\n",
    "\n",
    "The linear regression algorithm uses the **Least Squares Method** to minimize the error. The error (or loss function) is usually measured using the **Mean Squared Error (MSE)**, defined as:\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( Y_i \\) is the actual value of the dependent variable.\n",
    "- \\( \\hat{Y}_i \\) is the predicted value using the line equation.\n",
    "- `n` is the number of data points.\n",
    "\n",
    "The algorithm calculates the optimal `m` and `c` values that minimize this MSE.\n",
    "\n",
    "#### **Example Use Case**\n",
    "\n",
    "Let’s take a simple dataset where we want to predict a person’s **weight** (`Y`) based on their **height** (`X`). We have the following data:\n",
    "\n",
    "| Height (X) | Weight (Y) |\n",
    "|------------|------------|\n",
    "| 150 cm     | 50 kg      |\n",
    "| 160 cm     | 55 kg      |\n",
    "| 170 cm     | 65 kg      |\n",
    "| 180 cm     | 70 kg      |\n",
    "\n",
    "When plotted, a line that best fits these points would show how the weight increases linearly with height. Using the linear regression model, we can find the exact equation of this line and predict, say, the weight of a person who is 175 cm tall.\n",
    "\n",
    "#### **Steps to Implement Simple Linear Regression**\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Gather a dataset with an independent variable `X` and a dependent variable `Y`.\n",
    "  \n",
    "2. **Visualize the Data**:\n",
    "   - Plot the data to see if a linear relationship exists.\n",
    "\n",
    "3. **Calculate the Slope and Intercept**:\n",
    "   - Use the formula for slope \\( m \\) and intercept \\( c \\):\n",
    "\n",
    "\\[\n",
    "m = \\frac{n \\sum(XY) - \\sum(X) \\sum(Y)}{n \\sum(X^2) - (\\sum(X))^2}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "c = \\frac{\\sum(Y) - m \\sum(X)}{n}\n",
    "\\]\n",
    "\n",
    "4. **Fit the Line**:\n",
    "   - Use the `m` and `c` values to form the linear equation: \\( Y = mX + c \\).\n",
    "\n",
    "5. **Make Predictions**:\n",
    "   - Use the equation to predict `Y` for new values of `X`.\n",
    "\n",
    "6. **Evaluate the Model**:\n",
    "   - Calculate error metrics like **Mean Squared Error (MSE)** and **R-squared (R²)** to assess how well the model fits.\n",
    "\n",
    "#### **Implementation in Python**\n",
    "Here's a basic example using the `scikit-learn` library to implement simple linear regression:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data: X (Height in cm), Y (Weight in kg)\n",
    "X = np.array([150, 160, 170, 180]).reshape(-1, 1)\n",
    "Y = np.array([50, 55, 65, 70])\n",
    "\n",
    "# Create a Linear Regression model and train it\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Predict weight for a person with height = 175 cm\n",
    "height = np.array([[175]])\n",
    "predicted_weight = model.predict(height)\n",
    "\n",
    "# Print the slope (m) and intercept (c)\n",
    "print(f\"Slope (m): {model.coef_[0]}\")\n",
    "print(f\"Intercept (c): {model.intercept_}\")\n",
    "print(f\"Predicted weight for 175 cm height: {predicted_weight[0]} kg\")\n",
    "\n",
    "# Plotting the data points and the regression line\n",
    "plt.scatter(X, Y, color='blue')  # Scatter plot of original data points\n",
    "plt.plot(X, model.predict(X), color='red')  # Regression line\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Weight (kg)\")\n",
    "plt.title(\"Simple Linear Regression: Height vs Weight\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **Advantages of Simple Linear Regression**\n",
    "\n",
    "1. **Easy to implement and interpret**.\n",
    "2. Provides a good baseline for more complex models.\n",
    "3. Useful for identifying and modeling linear relationships.\n",
    "\n",
    "#### **Limitations of Simple Linear Regression**\n",
    "\n",
    "1. Assumes a linear relationship between variables.\n",
    "2. Sensitive to outliers, which can skew the results.\n",
    "3. Can only capture a single predictor variable; for multiple inputs, you need **Multiple Linear Regression**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95310730-ef97-44a5-957d-a0fe2a3ec35e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7562bb8-9623-4eac-babc-7525c0791238",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb9cba-5706-4081-85b0-0b7f5846005f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee7350-30b1-44bb-ac83-7119978c9bdf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defce2c-504f-424f-b497-ecad2314cdc1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495e375-53a4-4a35-926c-5495771519a0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a55914-3bf3-440b-9a25-ecb09caec7f9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dfaa5-ca56-45d2-86d2-ae7d08ac4f54",
   "metadata": {},
   "source": [
    "# Hierarchical Overview of Methods in Simple Linear Regression\n",
    "\n",
    "Simple linear regression involves multiple methods and concepts to build, fit, and evaluate the model. It is organized into **3 main stages**: **Model Representation**, **Model Fitting**, and **Model Evaluation**. Each stage involves several mathematical and statistical techniques to ensure a well-constructed model.\n",
    "\n",
    "Let's break it down hierarchically:\n",
    "\n",
    "### **1. Model Representation**\n",
    "This stage is about understanding the equation of the regression line and what the parameters represent.\n",
    "\n",
    "1.1 **Linear Equation**\n",
    "- The general equation is:\n",
    "\n",
    "\\[\n",
    "Y = mX + c\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- `Y` = Dependent variable (response/output).\n",
    "- `X` = Independent variable (predictor/input).\n",
    "- `m` = Slope of the line (change in Y for a unit increase in X).\n",
    "- `c` = Intercept (value of Y when X = 0).\n",
    "\n",
    "1.2 **Parameters to Learn**\n",
    "- In simple linear regression, we need to determine two parameters:\n",
    "  - **Slope (m)**: Determines the steepness and direction of the line.\n",
    "  - **Intercept (c)**: Sets the starting point of the line on the Y-axis.\n",
    "\n",
    "### **2. Model Fitting (Optimization Methods)**\n",
    "This stage involves calculating the best-fit line using optimization techniques. The goal is to **minimize the error** between predicted and actual values of `Y`. There are different ways to compute `m` and `c`:\n",
    "\n",
    "2.1 **Least Squares Method**\n",
    "- Minimizes the **sum of squared residuals** between actual and predicted values.\n",
    "- Residual is the **vertical distance** between the actual point and the regression line.\n",
    "- The formula for the slope (`m`) and intercept (`c`) are:\n",
    "\n",
    "\\[\n",
    "m = \\frac{n \\sum(XY) - \\sum(X) \\sum(Y)}{n \\sum(X^2) - (\\sum(X))^2}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "c = \\frac{\\sum(Y) - m \\sum(X)}{n}\n",
    "\\]\n",
    "\n",
    "2.2 **Gradient Descent**\n",
    "- An iterative optimization algorithm to find `m` and `c` by minimizing the cost function.\n",
    "- The cost function in linear regression is typically the **Mean Squared Error (MSE)**:\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
    "\\]\n",
    "\n",
    "  - The algorithm updates `m` and `c` until it finds values that minimize MSE using the formulae:\n",
    "\n",
    "\\[\n",
    "m = m - \\alpha \\frac{\\partial}{\\partial m} J(m, c)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "c = c - \\alpha \\frac{\\partial}{\\partial c} J(m, c)\n",
    "\\]\n",
    "\n",
    "  Where:\n",
    "  - `α` = Learning rate.\n",
    "  - `J(m, c)` = Cost function (MSE).\n",
    "\n",
    "2.3 **Normal Equation Method**\n",
    "- Directly computes `m` and `c` without iterations.\n",
    "- The formula for linear regression coefficients using matrices:\n",
    "\n",
    "\\[\n",
    "\\theta = (X^T X)^{-1} X^T Y\n",
    "\\]\n",
    "\n",
    "  Where:\n",
    "  - `θ` is a vector of parameters (`m` and `c`).\n",
    "  - `X` is the matrix of input features.\n",
    "  - `Y` is the vector of output values.\n",
    "\n",
    "### **3. Model Evaluation**\n",
    "Once the regression line is constructed, we need to evaluate how well it fits the data. There are several metrics and techniques to assess the model’s performance:\n",
    "\n",
    "3.1 **Error Metrics**\n",
    "- Used to quantify the difference between the actual and predicted values.\n",
    "\n",
    "  3.1.1 **Mean Squared Error (MSE)**:\n",
    "  \n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
    "\\]\n",
    "\n",
    "  - Measures the average squared difference between the predicted and actual values.\n",
    "  - Lower MSE indicates a better fit.\n",
    "\n",
    "  3.1.2 **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "\\[\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2}\n",
    "\\]\n",
    "\n",
    "  - RMSE is the square root of MSE. It represents the average prediction error in the same units as `Y`.\n",
    "  \n",
    "  3.1.3 **Mean Absolute Error (MAE)**:\n",
    "\n",
    "\\[\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i|\n",
    "\\]\n",
    "\n",
    "  - Measures the average of absolute differences between predicted and actual values.\n",
    "  - Less sensitive to outliers compared to MSE and RMSE.\n",
    "\n",
    "3.2 **Goodness-of-Fit Metrics**\n",
    "- These metrics help us understand how well the model explains the variance in the data.\n",
    "\n",
    "  3.2.1 **R-squared (R² Score)**:\n",
    "  \n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\sum (Y_i - \\hat{Y}_i)^2}{\\sum (Y_i - \\bar{Y})^2}\n",
    "\\]\n",
    "\n",
    "  - Indicates the proportion of variance in `Y` that is explained by `X`.\n",
    "  - Values range from 0 to 1. Closer to 1 indicates a better fit.\n",
    "\n",
    "  3.2.2 **Adjusted R-squared**:\n",
    "  \n",
    "\\[\n",
    "R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\n",
    "\\]\n",
    "\n",
    "  - Adjusted R² accounts for the number of predictors (`p`) and data points (`n`).\n",
    "  - Useful for multiple linear regression to prevent overfitting.\n",
    "\n",
    "3.3 **Residual Analysis**\n",
    "- Residuals are the differences between the actual and predicted values.\n",
    "  \n",
    "  3.3.1 **Residual Plot**:\n",
    "  - A scatter plot of residuals vs. predicted values.\n",
    "  - Helps detect patterns that indicate non-linearity, outliers, or heteroscedasticity (unequal variance).\n",
    "\n",
    "  3.3.2 **Durbin-Watson Test**:\n",
    "  - Checks for autocorrelation in residuals.\n",
    "  - Values close to 2 indicate no autocorrelation, while values closer to 0 or 4 suggest positive or negative autocorrelation, respectively.\n",
    "\n",
    "3.4 **Hypothesis Testing (Significance Testing)**\n",
    "- Determines if the independent variable significantly impacts the dependent variable.\n",
    "\n",
    "  3.4.1 **t-Test for Slope**:\n",
    "  - Tests if the slope (`m`) is significantly different from zero.\n",
    "  \n",
    "  3.4.2 **p-value**:\n",
    "  - Indicates the probability that the observed relationship occurred by chance.\n",
    "  - If `p < 0.05`, the relationship is considered statistically significant.\n",
    "\n",
    "### **Hierarchical Summary**\n",
    "1. **Model Representation**\n",
    "   - Linear Equation: `Y = mX + c`\n",
    "   - Parameters to Learn: Slope (`m`), Intercept (`c`)\n",
    "\n",
    "2. **Model Fitting (Optimization Methods)**\n",
    "   - Least Squares Method\n",
    "   - Gradient Descent\n",
    "   - Normal Equation Method\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Error Metrics: MSE, RMSE, MAE\n",
    "   - Goodness-of-Fit: R² Score, Adjusted R²\n",
    "   - Residual Analysis: Residual Plot, Durbin-Watson Test\n",
    "   - Hypothesis Testing: t-Test, p-value\n",
    "\n",
    "### **Final Thoughts**\n",
    "This hierarchical structure provides a systematic way to understand and implement linear regression. Each stage builds upon the previous one, from defining the model equation to optimizing it, and finally evaluating its effectiveness using a range of metrics and statistical tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cced37-36e6-4765-b0c3-bb917f637533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e10ce-0a35-4731-abc3-1c04e061f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81a384-e585-49fe-b57f-838a641d4df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372903d8-049b-487e-afe2-20c3863f0160",
   "metadata": {},
   "source": [
    " Let's dive deeper into the **Model Fitting** and **Model Evaluation** methods used in simple linear regression. We will explore each method's mathematical foundation and its importance from analytical, mathematical, and general perspectives.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Model Fitting (Optimization Methods)**\r\n",
    "\r\n",
    "#### **1. Least Squares Method**\r\n",
    "The least squares method is the most common approach to fit a linear regression model. It minimizes the sum of the squared differences (residuals) between observed values and predicted values.\r\n",
    "\r\n",
    "- **Mathematical Explanation**:\r\n",
    "  - Given a set of data points \\((X_i, Y_i)\\), the residual for each observation is:\r\n",
    "  \r\n",
    "  \\[\r\n",
    "  e_i = Y_i - (mX_i + c)\r\n",
    "  \\]\r\n",
    "\r\n",
    "  - The objective is to minimize the **Sum of Squared Errors (SSE)**:\r\n",
    "\r\n",
    "  \\[\r\n",
    "  SSE = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (Y_i - (mX_i + c))^2\r\n",
    "  \\]\r\n",
    "\r\n",
    "  - The optimal values of `m` and `c` are found by taking partial derivatives with respect to `m` and `c`, setting them to zero, and solving the resulting equations.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: Provides a robust foundation for understanding how well the model fits the data. The least squares criterion is well-established and has strong theoretical support in statistics.\r\n",
    "  - **Mathematical Perspective**: The method is computationally efficient and has closed-form solutions. It’s often preferred for its simplicity and ease of interpretation.\r\n",
    "  - **General Perspective**: This method is intuitive; it seeks to find the line that minimizes the distance from all data points, making it easy to understand for non-experts.\r\n",
    "\r\n",
    "#### **2. Gradient Descent**\r\n",
    "Gradient descent is an iterative optimization algorithm used to find the minimum of a function, commonly applied in machine learning for fitting models.\r\n",
    "\r\n",
    "- **Mathematical Explanation**:\r\n",
    "  - Start with initial values for `m` and `c`.\r\n",
    "  - Update the parameters in the opposite direction of the gradient of the cost function (e.g., Mean Squared Error):\r\n",
    "\r\n",
    "\\[\r\n",
    "m := m - \\alpha \\frac{\\partial J(m, c)}{\\partial m}\r\n",
    "\\]\r\n",
    "\\[\r\n",
    "c := c - \\alpha \\frac{\\partial J(m, c)}{\\partial c}\r\n",
    "\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- \\(\\alpha\\) = Learning rate (controls the size of the step).\r\n",
    "- \\(J(m, c)\\) = Cost function, typically MSE.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: Gradient descent allows for model fitting in high-dimensional spaces where closed-form solutions may not exist. It’s essential for training more complex models like neural networks.\r\n",
    "  - **Mathematical Perspective**: Offers flexibility in the choice of cost functions and can be adapted for various optimization problems. However, it requires careful tuning of the learning rate to ensure convergence.\r\n",
    "  - **General Perspective**: Although more complex than least squares, understanding gradient descent is crucial for modern machine learning applications, making it a valuable concept for those interested in data science.\r\n",
    "\r\n",
    "#### **3. Normal Equation Method**\r\n",
    "The normal equation provides a direct way to compute the parameters of a linear regression model without iteration.\r\n",
    "\r\n",
    "- **Mathematical Explanation**:\r\n",
    "  - The parameters are computed using matrix notation:\r\n",
    "\r\n",
    "\\[\r\n",
    "\\theta = (X^T X)^{-1} X^T Y\r\n",
    "\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- \\(\\theta\\) is a vector containing `m` and `c`.\r\n",
    "- \\(X\\) is the matrix of features, including a column of ones for the intercept.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: It provides an exact solution without the need for iteration, making it faster for small datasets.\r\n",
    "  - **Mathematical Perspective**: Offers an efficient way to derive coefficients when the dataset is not large, as the computational complexity grows with the number of features.\r\n",
    "  - **General Perspective**: While less intuitive than least squares, it showcases the power of linear algebra in solving regression problems quickly and accurately.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Model Evaluation**\r\n",
    "\r\n",
    "#### **1. Error Metrics**\r\n",
    "Error metrics help quantify the accuracy of the model's predictions.\r\n",
    "\r\n",
    "**1.1 Mean Squared Error (MSE)**\r\n",
    "- **Definition**: The average of the squares of the errors (residuals).\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\r\n",
    "\\]\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: MSE is widely used as a cost function in regression because it emphasizes larger errors due to squaring, providing a measure that penalizes significant deviations.\r\n",
    "  - **Mathematical Perspective**: MSE is mathematically convenient; its derivative is continuous, facilitating optimization.\r\n",
    "  - **General Perspective**: Provides a straightforward interpretation of the average prediction error, making it accessible to non-experts.\r\n",
    "\r\n",
    "**1.2 Root Mean Squared Error (RMSE)**\r\n",
    "- **Definition**: The square root of MSE, providing an error measure in the same units as `Y`.\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\r\n",
    "\\]\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: RMSE is useful for understanding the model's predictive accuracy in context since it’s in the same units as the dependent variable.\r\n",
    "  - **Mathematical Perspective**: RMSE offers a straightforward method to gauge error magnitude, ensuring sensitivity to larger errors.\r\n",
    "  - **General Perspective**: RMSE's intuitive units make it easier for stakeholders to understand the model's performance.\r\n",
    "\r\n",
    "**1.3 Mean Absolute Error (MAE)**\r\n",
    "- **Definition**: The average of the absolute differences between actual and predicted values.\r\n",
    "\r\n",
    "\\[\r\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i|\r\n",
    "\\]\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: MAE provides a robust measure of average error without emphasizing larger errors as much as MSE.\r\n",
    "  - **Mathematical Perspective**: MAE is easier to interpret, although it lacks differentiability at zero, which can complicate some optimization processes.\r\n",
    "  - **General Perspective**: Its simplicity makes it very relatable, as it conveys the average error in a very clear way.\r\n",
    "\r\n",
    "#### **2. Goodness-of-Fit Metrics**\r\n",
    "Goodness-of-fit metrics assess how well the model explains the variance in the dependent variable.\r\n",
    "\r\n",
    "**2.1 R-squared (R² Score)**\r\n",
    "- **Definition**: Proportion of the variance in `Y` explained by `X`.\r\n",
    "\r\n",
    "\\[\r\n",
    "R^2 = 1 - \\frac{\\sum (Y_i - \\hat{Y}_i)^2}{\\sum (Y_i - \\bar{Y})^2}\r\n",
    "\\]\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: R² provides a quick measure of model performance and interpretability of the variance explained.\r\n",
    "  - **Mathematical Perspective**: Offers a normalized measure (ranging from 0 to 1), which helps compare models with different numbers of predictors.\r\n",
    "  - **General Perspective**: R² is a familiar statistic; stakeholders often use it to gauge model effectiveness intuitively.\r\n",
    "\r\n",
    "**2.2 Adjusted R-squared**\r\n",
    "- **Definition**: Adjusts R² based on the number of predictors and sample size.\r\n",
    "\r\n",
    "\\[\r\n",
    "R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\r\n",
    "\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- `n` = number of observations.\r\n",
    "- `p` = number of predictors.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: Adjusted R² penalizes for adding irrelevant predictors, preventing misleading conclusions from inflated R² values in models with many predictors.\r\n",
    "  - **Mathematical Perspective**: Adjusted R² ensures a more accurate measure of model performance, particularly in multiple regression contexts.\r\n",
    "  - **General Perspective**: Useful for understanding whether adding more variables genuinely improves the model’s explanatory power, enhancing stakeholder trust in the results.\r\n",
    "\r\n",
    "#### **3. Residual Analysis**\r\n",
    "Residual analysis evaluates the errors in predictions to ensure model validity.\r\n",
    "\r\n",
    "**3.1 Residual Plot**\r\n",
    "- **Definition**: A scatter plot of residuals versus predicted values.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: Helps assess the assumptions of linear regression, such as homoscedasticity (constant variance of residuals) and linearity.\r\n",
    "  - **Mathematical Perspective**: Visualizes residual patterns that can indicate model misspecification.\r\n",
    "  - **General Perspective**: A straightforward tool for understanding model performance, helping identify potential issues or deviations from expected behavior.\r\n",
    "\r\n",
    "**3.2 Durbin-Watson Test**\r\n",
    "- **Definition**: A test for detecting autocorrelation in the residuals of a regression analysis.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: Ensures that residuals are independent; a key assumption in regression analysis.\r\n",
    "  - **Mathematical Perspective**: The test statistic ranges from 0 to 4, with a value around 2 indicating no autocorrelation.\r\n",
    "  - **General Perspective**: Provides a clear decision-making framework for identifying issues with residuals, which is vital for model credibility.\r\n",
    "\r\n",
    "#### **4. Hypothesis Testing**\r\n",
    "Hypothesis testing assesses the significance of the relationships in the model.\r\n",
    "\r\n",
    "**4.1 t-Test for Slope**\r\n",
    "- **Definition**: Tests if the slope of the regssion line (`m`) is significantly different from zero.\r\n",
    "\r\n",
    "\\[\r\n",
    "t = \\frac{m}{SE(m)}\r\n",
    "\\]\r\n",
    "\r\n",
    "Where \\( SE(m) \\) is the standard error of the slope.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Anal\r\n",
    "\r\n",
    "ytical Perspective**: Validates the relevance of the independent variable in predicting the dependent variable.\r\n",
    "  - **Mathematical Perspective**: Provides a formal statistical framework to assess parameter significance.\r\n",
    "  - **General Perspective**: Empowers stakeholders to make informed decisions based on whether the predictor has a meaningful relationship with the response variable.\r\n",
    "\r\n",
    "**4.2 p-value**\r\n",
    "- **Definition**: Represents the probability that the observed data would occur under the null hypothesis.\r\n",
    "\r\n",
    "- **Importance**:\r\n",
    "  - **Analytical Perspective**: A small p-value (typically < 0.05) suggests rejecting the null hypothesis, indicating a significant relationship.\r\n",
    "  - **Mathematical Perspective**: Provides a basis for statistical inference, linking empirical data with probability theory.\r\n",
    "  - **General Perspective**: Offers a clear threshold for decision-making, helping non-experts understand the significance of results in an intuitive manner.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Summary**\r\n",
    "The methods and metrics used in simple linear regression—ranging from fitting techniques like the least squares method and gradient descent to evaluation metrics like R² and hypothesis testing—are fundamental for building, assessing, and interpreting regression models. Each method serves specific purposes and provides insights from diffe.\n",
    "lity. This depth of understanding is critical for analysts, mathematicians, and stakeholders alike.\r\n",
    "\r\n",
    "If you have any further questions or need clarification on specific points, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a825ce7-825e-4380-b4d9-bf64aa690e6f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86f90e-6b16-4588-ad75-a282684c6698",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37409fd6-9c0f-42a6-b2cc-6c51e2b235a3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fff7f7-45fe-43b5-8adf-f3bf798d42ff",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb57bf1-bdb6-4193-a01c-6c100e7b8f18",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549e994-537d-40c4-802f-489de3180be6",
   "metadata": {},
   "source": [
    "Here's the complete and structured hierarchy of linear regression, covering all aspects from **model fitting** to **evaluation**, **diagnostics**, and **hypothesis testing**. This will serve as a comprehensive guide, giving a clear overview of the various components and their roles within linear regression analysis.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Hierarchy of Linear Regression Analysis**\r\n",
    "\r\n",
    "### 1. **Model Fitting (Optimization Methods)**\r\n",
    "Model fitting is the process of determining the optimal parameters for the regression model. Several methods are used to find the best line that minimizes errors.\r\n",
    "\r\n",
    "- **1.1 Least Squares Method**\r\n",
    "  - Minimizes the sum of squared differences between observed values and predicted values.\r\n",
    "  - Mathematical approach: Closed-form solution using matrix algebra.\r\n",
    "  \r\n",
    "- **1.2 Gradient Descent**\r\n",
    "  - An iterative optimization method used when the number of parameters is large or in complex models.\r\n",
    "  - Minimizes the cost function by updating parameters iteratively based on the gradient.\r\n",
    "\r\n",
    "- **1.3 Normal Equation Method**\r\n",
    "  - A direct method for finding the least squares solution using a formula.\r\n",
    "  - Useful for simple linear regression or when the dataset is not too large.\r\n",
    "\r\n",
    "### 2. **Model Evaluation**\r\n",
    "Model evaluation quantifies how well the regression model fits the data and assesses its predictive power.\r\n",
    "\r\n",
    "#### **2.1 Error Metrics**  \r\n",
    "Error metrics evaluate the accuracy of predictions by measuring the distance between observed and predicted values.\r\n",
    "  \r\n",
    "- **2.1.1 Mean Squared Error (MSE)**\r\n",
    "  - Measures the average squared difference between observed and predicted values.\r\n",
    "  \r\n",
    "- **2.1.2 Root Mean Squared Error (RMSE)**\r\n",
    "  - Square root of MSE, indicating the average error in the units of the dependent variable.\r\n",
    "\r\n",
    "- **2.1.3 Mean Absolute Error (MAE)**\r\n",
    "  - Measures the average absolute difference between observed and predicted values.\r\n",
    "\r\n",
    "#### **2.2 Goodness-of-Fit Metrics**\r\n",
    "These metrics assess the proportion of variance explained by the model and how well the regression line represents the data.\r\n",
    "\r\n",
    "- **2.2.1 Total Sum of Squares (TSS)**\r\n",
    "  - Total variance in the observed \\(Y\\) values.\r\n",
    "  \r\n",
    "- **2.2.2 Explained Sum of Squares (ESS)**\r\n",
    "  - Portion of the total variance explained by the regression model.\r\n",
    "\r\n",
    "- **2.2.3 Residual Sum of Squares (RSS)**\r\n",
    "  - Variance in \\(Y\\) not explained by the model (sum of squared residuals).\r\n",
    "\r\n",
    "- **2.2.4 R-squared (R² Score)**\r\n",
    "  - Proportion of total variance explained by the independent variables:\r\n",
    "\r\n",
    "    \\[\r\n",
    "    R^2 = 1 - \\frac{RSS}{TSS}\r\n",
    "    \\]\r\n",
    "\r\n",
    "- **2.2.5 Adjusted R-squared**\r\n",
    "  - Adjusted for the number of predictors; penalizes for overfitting:\r\n",
    "\r\n",
    "    \\[\r\n",
    "    \\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\r\n",
    "    \\]\r\n",
    "\r\n",
    "- **2.2.6 F-Statistic**\r\n",
    "  - Tests whether the model explains a significant amount of variance compared to a model with no predictors:\r\n",
    "\r\n",
    "    \\[\r\n",
    "    F = \\frac{(ESS / k)}{(RSS / (n - k - 1))}\r\n",
    "    \\]\r\n",
    "\r\n",
    "- **2.2.7 Residual Standard Error (RSE)**\r\n",
    "  - Measures the average distance that the observed values fall from the regression line:\r\n",
    "\r\n",
    "    \\[\r\n",
    "    RSE = \\sqrt{\\frac{RSS}{n - p}}\r\n",
    "    \\]\r\n",
    "\r\n",
    "### 3. **Residual Analysis**\r\n",
    "Residual analysis is used to diagnose the goodness-of-fit and assumptions of the linear regression model.\r\n",
    "\r\n",
    "- **3.1 Residual Plots**\r\n",
    "  - Scatterplot of residuals versus predicted values.\r\n",
    "  - Detects non-linearity, heteroscedasticity, and outliers.\r\n",
    "\r\n",
    "- **3.2 Normal Q-Q Plot**\r\n",
    "  - Assesses if residuals are normally distributed.\r\n",
    "\r\n",
    "- **3.3 Standardized Residuals**\r\n",
    "  - Residuals scaled by their standard deviation to detect extreme observations.\r\n",
    "\r\n",
    "- **3.4 Durbin-Watson Test**\r\n",
    "  - Tests for autocorrelation in the residuals (especially for time-series data).\r\n",
    "\r\n",
    "### 4. **Model Assumptions**\r\n",
    "Linear regression models rely on several key assumptions. Violation of these assumptions can lead to biased or misleading results.\r\n",
    "\r\n",
    "- **4.1 Linearity**\r\n",
    "  - Relationship between \\(X\\) and \\(Y\\) should be linear.\r\n",
    "\r\n",
    "- **4.2 Independence**\r\n",
    "  - Observations must be independent of each other.\r\n",
    "\r\n",
    "- **4.3 Homoscedasticity**\r\n",
    "  - Constant variance of errors across all levels of \\(X\\).\r\n",
    "\r\n",
    "- **4.4 Normality of Errors**\r\n",
    "  - Residuals should be normally distributed.\r\n",
    "\r\n",
    "- **4.5 No Multicollinearity** (for Multiple Linear Regression)\r\n",
    "  - Independent variables should not be highly correlated with each other.\r\n",
    "\r\n",
    "### 5. **Hypothesis Testing**\r\n",
    "Hypothesis testing is used to determine the significance of the relationship between independent and dependent variables.\r\n",
    "\r\n",
    "#### **5.1 t-Test**\r\n",
    "  - Tests whether each individual regression coefficient (\\(\\beta_i\\)) is significantly different from zero.\r\n",
    "  - **Null Hypothesis**: \\(\\beta_i = 0\\).\r\n",
    "\r\n",
    "  \\[\r\n",
    "  t = \\frac{\\beta_i}{SE(\\beta_i)}\r\n",
    "  \\]\r\n",
    "\r\n",
    "#### **5.2 F-Test**\r\n",
    "  - Tests the overall significance of the regression model.\r\n",
    "  - **Null Hypothesis**: All regression coefficients are zero (\\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\)).\r\n",
    "\r\n",
    "#### **5.3 p-value**\r\n",
    "  - Probability of obtaining a test statistic at least as extreme as the one observed, under the null hypothesis.\r\n",
    "  - If \\( p < \\alpha \\) (typically 0.05), reject the null hypothesis.\r\n",
    "\r\n",
    "### 6. **Model Selection and Tuning**\r\n",
    "Selecting the best model configuration involves choosing the right predictors and avoiding overfitting or underfitting.\r\n",
    "\r\n",
    "- **6.1 Feature Selection Techniques**\r\n",
    "  - Forward Selection, Backward Elimination, Stepwise Selection.\r\n",
    "\r\n",
    "- **6.2 Cross-Validation**\r\n",
    "  - Splitting the data into training and testing sets to evaluate the model’s performance.\r\n",
    "\r\n",
    "- **6.3 Regularization Techniques**\r\n",
    "  - **Ridge Regression**: Adds L2 penalty to prevent overfitting.\r\n",
    "  - **Lasso Regression**: Adds L1 penalty to perform feature selection.\r\n",
    "\r\n",
    "### 7. **Interpretation and Communication**\r\n",
    "Effective communication of regression results is crucial for stakeholders to understand the implications of the findings.\r\n",
    "\r\n",
    "- **7.1 Coefficient Interpretation**\r\n",
    "  - Explain the impact of each predictor on the dependent variable.\r\n",
    "  \r\n",
    "- **7.2 Confidence Intervals**\r\n",
    "  - Indicates the range within which the true regression coefficients are likely to lie.\r\n",
    "\r\n",
    "- **7.3 Predictive Power**\r\n",
    "  - Use the model to predict new data and assess its reliability.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Summary**\r\n",
    "\r\n",
    "This hierarchy comprehensively covers all aspects of linear regression, providing a structured way to understand the different phases—from model fitting to evaluation, diagnostic testingle in ensuring that the linear regression model is correctly specified and evaluated.\r\n",
    "\r\n",
    "If you’d like to explore any specific area in detail, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3b8eb-5460-448d-8e03-c3b3f4182f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
